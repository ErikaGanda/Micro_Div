---
title: "DADA2 for high throughput sequence processing"
subtitle: "Written by Gordon Custer 2018"
output: html_notebook
---

Hopefully from reading the associated literature you know that what makes DADA2 so interesting and revolutionary is the fact that DADA2 can learn errors in sequence data and correct these errors for indivdual sequences. This allows researchers to be sure when a sequence is infact a unique biological variant versus a sequencing error. In addition to this novel error learning step, sequences are grouped into exact sequence variants (ESV) as opposed to being grouped at some level of similarity, as was the norm prior to the development of DADA2 and other recent pipelines, like "Usearch". Grouping of taxa at a predefined level of similarity has the potential to mask the underlying structure of the data. By utilizing ESVs we are able to know true abundances of individuals and avoid lumping of potentially distinct taxa. Thse advances allows researchers to examine their data more thoroughly and uncover trends which may be masked at the classic 97% otu clustering.  

DADA2 offers a great tutorial. You should familiarize yourself with it. Much of this tutorial is borrowed from theirs. A link to this can be found under citation 3. 

As mentioned before, DADA2 is both unique and a better option than some of the other bioinformatic pipelines due to the implementation of the DADA error learning algorithm. This algorithm infers whether or not individual base pairs are sequencing errors or a true biological variant. In the simplest terms, this is accomplished by pooling all samples and looking for sequences which occur across the entire experiment. If a sequence only occurs once throughout the entire experiment then the changes of it being a true biological variant are lower than if it were observed more times. This "magic" happens in step 3- "Infer sample composition". We will further examine this shortly. Another huge benefit to DADA2 is that it is all in R. Running a bioinformatic pipeline locally can be very demanding in terms of computing power. However, DADA2 is optimized to run on your computer and requires a relatively small amount of computing power. You have the option to run this pipeline on any system with R installed, this includes the new Teton cluster. While a super comptuer is an option, I find it easier to work in R Studio which isn't available on Teton. 

##Overview of the DADA2 workflow
Below are the main steps for the processing of high throughput sequence data. Each step has a more detailed section below and is accompanied by the necessary code. I highly recommend consulting the help pages for each function so you can further explore the options offered. Only in doing so will you know exactly what you are doing. 

1. Filter and Trim: fastqFilter() or fastqPairedFilter()
2. Dereplicate: derepFastq()
3. Infer sample composition: dada()
4. Merge paired reads: mergePairs()
5. Make sequence table: makeSequenceTable()
6. Remove chimeras: isBimeraDenovo() or removeBimeraDenovo()

The end product of the DADA2 bioinformatic pipeline is a "sequence variant table" (very similar to the classic operational taxonimc unit table or OTU table) and taxonomy table which can be imported into Phyloseq for further downstream analysis (in the next tutorials). These two pipelines merge seamlessly and provide researchers with a friendly transition. 

The example dataset we will be working with can be found on your desktop. Copy it to your folder for analysis. 

##Getting started
Dada2 assumes the following has been addressed prior to starting the workflow:

1. Samples have been demultiplexed. This means that the reads have been split into individual .fastq files. The sequencing facility often does this for you. If your samples have not been demultiplexed, commands such as split_libraries_fastq.py > split_sequence_file_on_sample_ids.py exist and can do the heavy lifting for you. These exist in QIIME (another bioinformatics pipeline). If you need to use these commands the QIIME help pages are extremely useful. Look at the folder containing your sequences. Have they been sperated?

2. Non-biological nucleotides have been removed. This means primers, adapters, linkers, etc. have been removed from the reads, leaving us with the biologically relevant portion. Programs such as "trimmomatic" can be used for this. In many cases this step has been done or you as is the case with our reads here. 

3. And finally, if we are working with paired end data, the forward and reverse files contain matching reads. That is if "sample 1" has a forward read file it must also have a reverse reads file. Look to make sure each forward read has its reverse complement.  

After we have confirmed that our reads satisfy the three prerequisites, we can proceed with the DADA2 pipeline for processing samples. 

##Setting up your session
OK first let's make sure our installation of DADA2 was successful and check which version of the package we have installed. 

Exercise 1: Load the DADA2 package. You can do this in a new chunk of code or in the console. If you do it in the console however, it will not be saved for next time you wish to load the package. I recommend loading packages in the top of your script so its always there for you. In your handout, copy the descritption of the DADA2 package from the help page. Next, we want to check which version of DADA2 we have installed, record which version you have installed in your handout. 

```{r}

packageVersion()
```

For future reference, this is important for other researchers who may want to duplicate our efforts. It is good practice to have an excel workbook and to record all of this information there. I will tell you other times you will want to record information in this excel workbook. Create an excel workbook and place this information in it. Save the workbook to your desktop folder.

###The DADA2 pipeline
In the first step, we want to remove reads that do not meet a certain quality threshold. The quality threshold represents how sure we are that the basepair is actually the basepair it was assigned by the sequencing facility. In order to do so, the first step is to visually inspect the read profiles. This plot shows the average quality score at each base pair across our reads. The most common metric used is known as a phred score. Wikipedia has a good explanation of phred score. I encourage you to take 5 minutes to read that page. 

Exercise 2: You will need to change your working directory to the folder containing your raw reads. Please do so and record this in your handout. 
```{r}
# File parsing
#As mentioned earlier, DADA2 assumes the reads are split into forward and reverse. You will want to make two folders, one named "forward" and the other "reverse". You want to place all forward reads in the forward folder and the same for the reverse reads. The first two lines here tell R where it can find the reads. 
pathF <- "/Users/gordoncuster/Desktop/Micro_Div/Reads/Foward" # CHANGE ME to the directory containing your demultiplexed forward-read fastq files
pathR <- "/Users/gordoncuster/Desktop/Micro_Div/Reads/Reverse" # CHANGE ME ...

#This next section createds a new folder inside the forward and reverse folders for the filtered reads. This is where you will be working from once you have filtered and trimmed the raw reads. 
filtpathF <- file.path(pathF, "filtered") # Filtered forward files go into the pathF/filtered/ subdirectory
filtpathR <- file.path(pathR, "filtered") # ...
fastqFs <- sort(list.files(pathF, pattern="fastq.gz", full.names = T))
fastqRs <- sort(list.files(pathR, pattern="fastq.gz", full.names = T))
```

You should examine your quality plots prior to setting the truncLen=c(,) argument. This will need to be specific for you analysis. I think it sufficient to view 5 or so of your quality proflies. Do this for both forward and reverse reads. The truncLen parameter of the filterAndTrim function will cut your reads at a set length. The first number corresponds to the forward reads, and the second number corresponds to the reverse reads.

Exercise 3: View your quality plots. I generally cut a read when the phred score falls below ~20. Where would you cut your forward reads and where would you cut your reverse reads? Record your answer in your handout. 
```{r}
plotQualityProfile(fastqFs[1:5])
plotQualityProfile(fastqRs[1:5])
```

Note: While bacterial (16S) reads are typically the same length +/- a few base pairs, Fungal ITS reads can greatly vary in length. Sometimes the difference can be 300 bp over the entire ITS operon. When processing ITS reads it is best to remove the trunclen() option. Leaving it will remove any reads which do not meet the minimum length specified and will result in many reads being rejected even if they are of good quality. Instead of using a minimum length we recomend filtering reads based on quality. We will delve further into this shortly. 

```{r}
#You will have to rerun this in order to get the filter and trip parameter to work. This is an artifact of plotting the quality scores. 
fastqFs <- sort(list.files(pathF, pattern="fastq.gz"))
fastqRs <- sort(list.files(pathR, pattern="fastq.gz"))
if(length(fastqFs) != length(fastqRs)) stop("Forward and reverse files do not match.")
```

Ok now for the good stuff. This is the first step we have encountered in the DADA2 pipeline that has many options for customization and as such, the first step which requires you to fully understand what the options mean and to implement them.

Exercise 4: View the help page for the filterAndTrim() function. Explore the available options and customize your filtering step to accurately and effectively trim and filter your sequence data based on the plots you viewed earlier. What do the maxN and truncQ options do? Look at the truncLen() option. Do not run this code. This chunk will take a long time to run. We have provided you the output so you can continue in the tutorial.
```{r}
track.filt<-filterAndTrim(fwd=file.path(pathF, fastqFs), filt=file.path(filtpathF, fastqFs),
              rev=file.path(pathR, fastqRs), filt.rev=file.path(filtpathR, fastqRs),
              truncLen=c(), maxEE=2, truncQ=11, maxN=, rm.phix=TRUE,
              compress=TRUE, verbose=TRUE, multithread=TRUE)
```

Look at the track.filt object; it will contain two columns. This is used to keep track of the number of reads remaining after each step. You will always want to keep track of your reads through processing. If you are too stringent with one of your parameters too many of your reads could be discarded. On the other hand, if you are not stringent enough erroneous sequences could be included in downstream analysis. Further down in this script there is a chunk of code which tracks all the reads through the workflow. 

In this section we are setting up the paths to the filtered files produced the previous step. This is similar to the section above in which you set the original path to the raw reads. The first two lines here change the path to the filtered reads. 

Exercise 5: Change the pathway in lines 101 and 102 to your desktop folders. Copy and paste the pathways to your forward and reverse folders into the lab handout. 
```{r}
# File parsing
filtpathF <- "/Users/gordoncuster/Desktop/Micro_Div/Reads/Foward/filtered" # CHANGE ME to the directory containing your filtered forward fastq files
filtpathR <- "/Users/gordoncuster/Desktop/Micro_Div/Reads/Reverse/filtered" # CHANGE ME ...
filtFs <- list.files(filtpathF, pattern="fastq.gz", full.names = TRUE)
filtRs <- list.files(filtpathR, pattern="fastq.gz", full.names = TRUE)
sample.names <- sapply(strsplit(basename(filtFs), "_"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
sample.namesR <- sapply(strsplit(basename(filtRs), "_"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
#The ! reads is not. So this line below reads "if the sample.names and sample.namesR are not identical than stop"
#This is a final check to ensure the data exists in forward and reverse reads for the same samples. 
if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files do not match.")
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Here is where the "DADA2 magic" happens. The learnErrors() step uses machine learning to uncover the true underlying proportion of errors. This is accomplished by alternating the estimation of the error rate and the inference of the sample composition until they converge. Look at the help page and see what the authors have to say about this step. 

You will not need to run this chunk. We have provided the outputs for you. This step can take a long time. 
```{r}
#set.seed is used for reproducibilty 
set.seed(100)
# Learn forward error rates
errF <- learnErrors(filtFs, nread=1e6, multithread=TRUE)
# Learn reverse error rates
errR <- learnErrors(filtRs, nread=1e6, multithread=TRUE)
#these plots allow you to see the error rates for each possible transition. 
plotErrors(errF)
plotErrors(errR)
```

This next step merges forward and reverse sequences into a single read and keeps track of the abundance of each read. If you were to work with only forward or only reverse reads then you could only infer so much; This includes taxonomic identity. Think of it like this. If you compared the first 5 numbers in the serial number of a one dollar bill printed in the same day there is a fairly good chance that they are the same. However, we all know that no two dollar bills have the exact same serial number. If you wanted to "know" (I put know in quotations because I really mean have proof) that these two bills were different you could look at the entire serial number. This is also true for sequence reads. If you had two closely related species, then you will want as long of a stretch of DNA as possible to determine the differences between the individuals. In merging the forward and reverse sequences, we are effectively allowing researchers to see the "whole serial number" or the longest possible stretch of DNA. 

This step also helps to reduce computing time; in that it removes duplicate sequences, greatly reducing the size of the data set. The dada() step infers the number of truly unique sequences present in the data set. This, along with the learn errors step, is the most important and unique step of the DADA2 pipeline. Again you will not need to run the code for this as it takes a long time. We have provided the output from this section for you. 

```{r}
# Sample inference and merger of paired-end reads
mergers <- vector("list", length(sample.names))
names(mergers) <- sample.names
for(sam in sample.names) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=TRUE)
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR)
    mergers[[sam]] <- merger
}
rm(derepF); rm(derepR)
```

Here we take the merged reads from above and transform the data into a sequence table. This is the classic format for storing processed HTS data. A sequence variant table is compact and contains the information necessary for further analysis. No matter which pipeline you use, you will come to a step such as this. Others call their final table an OTU table because instead of sequence variants, they use OTUs grouped at some level of sequence similarity. The final product is the same, a table consisting of sequence counts by sites. 
```{r}
seqtab <- makeSequenceTable(mergers)
#But be weary! This sequence table still could contain non-biological sequnces, in the form of chimeras!
```

What is a chimera? A chimera is a mythological Greek creature which is part lion, part goat, part serpent and is capable of breathing fire... What does that have to do with sequence data? Well, in the PCR performed prior to sequencing, amplicons are sometimes halted in the middle of an elongation cycle. If this is the case, at the beginning of the next cycle a different sequence may become attached and elongated. Therefor, a chimera comes into existence. A chimera (in the HTS sense) is a sequence made up of parts to two parent sequences. The sequence does not represent a biological variant and as such should be removed from the data set prior to downstream analysis.

```{r}
seqtabNoC <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)
```

Once you have built the Sequence tab let's take a look at it. This data table could be huge so lets use the skills we learned to view only the first 10 columns. Remember View(Pima[,1:4])?

Exercise 6: View the first 5 columns of your sequence table. Copy the code you use into your handout.

As mentioned above, it is VERY important to track the number of reads retained after each step throughout the pipeline. This chunk of code allows you to track the number of reads. While there is no magic number or percentage of reads retained, you will want to address this on a data set by data set basis. Consider your end goal prior to choosing the desired number of retained sequences. Can you think of an example when you would want to be very stringent with the reads retained? What about an example of when you wouldn't care quite as much?

This is a great example of data you might want to save in your excel workbook.

Exercise 7: Record the number of sequences retained through the entire pipline in your handout. Hint use the function colSums().

```{r}
#remember the Unique() function? This is another variation of it. 
getNreads <- function(x) sum(getUniques(x))
track <- cbind(track.filt, sapply(mergers, getNreads), rowSums(seqtab), rowSums(seqtabNoC))
colnames(track) <- c("input", "filtered", "merged", "tabled", "nonchim")
track
colSums()
```

Now that we have our sequence table and have seen that each column is headed by the sequence that represents that variant, we want to assign taxonomy. Assigning taxonomy is extremely useful when trying to interpret what is happening in the biological sense after some treatment. In this step, we tell the assignTaxonomy() function several things. The first, which sequence table to use. We want it to use the table with no chimeras as this is our best "guess" at the true underlying proportions of microbes. Second, we tell the function where to find the database. Many different databases exist so you will want to figure out which one best suits your needs. Information on available databases can be found at https://benjjneb.github.io/dada2/training.html. 

Exercise 8: You will want to download the Silva 132 database and move it to your Desktop folder. Then change the pathway in the function to the correct folder. Finally, the minBoot argument specifies the minimum bootstrapping support required for taxonomic classification. Higher minboot levels increase confidence in the taxonomic assignment. Higher confidence levels are wanted when dealing with ITS reads than with bacteria. This is an artifact of the underlying structure of ITS reads vs. 16S reads. This should be assigned on a per experiment basis. 

You will not need to run this chunk of code. We have provided the output from this section for you. 
```{r}
# Assign taxonomy
tax <- assignTaxonomy(seqtabNoC, "", minBoot = 70, multithread=TRUE)
```

Exercise 9: View the first 10 rows of your taxonomy table. What families are represented? Record this in your handout along with the code you used to get this. 

```{r}

```

Finally, some analyses may benefit from the inclusion of a phylogenetic tree. While this option is available you should question whether or not you actually want to use it. In many cases, a tree of the entire data set is not interpretable and provides little or no usable information. It may be best to subset your data set to only include a subset of taxa prior to building your tree. This is extremely computationally intensive and as such, can take many hours or even days to build. This is true even if it is done on TETON (partially because this function does not support parallel processing). I recommend reading the help pages for these functions but not actually running it unless absolutely necessary. 

```{#r}
library(DECIPHER)
seqs <- getSequences(seqtabNoC)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA,verbose=FALSE)

phangAlign <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phangAlign)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phangAlign)
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
        rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)
```

In the end, we have filtered and trimmed our data, learned errors, removed and or fixed non-biologically relevant variants, removed chimeras, created a sequence variant table, and assigned taxonomy. We now have all the necessary pieces for downstream analysis. The next tutorials utilizes the packages Phyloseq and the following, Vegan, for downstream analysis of our data. 

Exercise 10: Install the phyloseq package. Search google for instructions.2

```{r}

```

Make sure you save your environment for future work. You wouldnt want to have to re-run this entire pipeline everytime you wished to examine your data. 

Citations and Resources:
1: https://www.r-project.org/ 
2: https://benjjneb.github.io/dada2/index.html
3. https://benjjneb.github.io/dada2/tutorial.html